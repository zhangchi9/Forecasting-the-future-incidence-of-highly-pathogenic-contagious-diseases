{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, OrderedDict\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grids(object):\n",
    "    # An class to get region from coordinate\n",
    "    def __init__(self, data_path, city_list):\n",
    "        super(Grids, self).__init__()\n",
    "        self.grid_dict = {}\n",
    "\n",
    "        for city in city_list:\n",
    "            filename = os.path.join(data_path, \"city_%s\" % city, \"grid_attr.csv\")\n",
    "            df = pd.read_csv(filename, header=None, names=[\"grid_x\", \"grid_y\", \"region_id\"])\n",
    "            df['region_id'] = city + '_' + df['region_id'].astype('str')\n",
    "            df = df.groupby('region_id').agg({'grid_x':['min','max'], 'grid_y':['min','max']})\n",
    "            df = df.reset_index()\n",
    "            df.columns = ['region_id', 'x_min', 'x_max', 'y_min', 'y_max']\n",
    "            self.grid_dict[city] = df\n",
    "\n",
    "        self.df_all = pd.concat([df for df in self.grid_dict.values()])\n",
    "\n",
    "    def get_region(self, x, y, city=None):\n",
    "        x = float(x)\n",
    "        y = float(y)\n",
    "        query = \"(x_min <= @x) and (@x <= x_max) and (y_min <= @y) and (@y <= y_max)\"\n",
    "        if city:\n",
    "            df = self.grid_dict[city]\n",
    "            region = df.query(query)\n",
    "            region = region.iloc[0,0] if len(region) else None\n",
    "        else:\n",
    "            region  = self.df_all.query(query)\n",
    "            region = region.iloc[0,0] if len(region) else None\n",
    "        return region\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func, n_cores=1):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = mp.Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_region(df):\n",
    "    city = df['city'].iloc[0]\n",
    "    df_city = grids_table.grid_dict[city]\n",
    "    df_list = []\n",
    "    for region_id in df_city['region_id']:\n",
    "        x_min = df_city.query(f\"region_id=='{region_id}'\")['x_min'].iloc[0]\n",
    "        x_max = df_city.query(f\"region_id=='{region_id}'\")['x_max'].iloc[0]\n",
    "        y_min = df_city.query(f\"region_id=='{region_id}'\")['y_min'].iloc[0]\n",
    "        y_max = df_city.query(f\"region_id=='{region_id}'\")['y_max'].iloc[0]\n",
    "        query = \"(@x_min <= grid_x) and (grid_x <= @x_max) and (@y_min <= grid_y) and (grid_y <= @y_max)\"\n",
    "        df_region = df.query(query).copy()\n",
    "        df_region['region'] = region_id\n",
    "        df_list.append(df_region)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_process(data_path, city_list, output_path):\n",
    "    res = []\n",
    "    for i, city in enumerate(city_list):\n",
    "        filename = os.path.join(data_path, \"city_%s\" % city, \"density.csv\")\n",
    "        density = pd.read_csv(filename, \n",
    "                                sep=',', \n",
    "                                header=None,\n",
    "                                names=[\"date\", \"hour\", \"grid_x\", \"grid_y\", \"density\"])\n",
    "        # return density\n",
    "        density = density.groupby(['date', \"grid_x\", \"grid_y\"])['density'].median().reset_index()\n",
    "        density['city'] = city\n",
    "        \n",
    "        n_cores = mp.cpu_count() - 1\n",
    "        density = parallelize_dataframe(density, make_region, n_cores=n_cores)\n",
    "\n",
    "        density = density.groupby(['date', \"region\"])['density'].sum().reset_index()\n",
    "        density = density.pivot_table(values='density', \n",
    "                                        index=density['date'], \n",
    "                                        columns='region', \n",
    "                                        aggfunc='first').reset_index()\n",
    "        res.append(density)\n",
    "\n",
    "    df = reduce(lambda left, right: pd.merge(left, right, on='date', how='outer'), res)\n",
    "    df.to_csv(os.path.join(output_path, 'density.csv'), index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_int(output_path, date_range):\n",
    "    filename = os.path.join(output_path, \"density.csv\")\n",
    "    \n",
    "    df_dens = pd.read_csv('dataset/data_processed/density.csv')\n",
    "    df_dens['date'] = pd.to_datetime(df_dens['date'], format='%Y%m%d')\n",
    "    df_dens = df_dens.set_index('date')\n",
    "\n",
    "    df_dens_int = df_dens.reindex(date_range, fill_value=None)\n",
    "    df_dens_int = df_dens_int.interpolate(method='quadratic',\n",
    "                                          limit_direction='both')\n",
    "    df_dens_int = df_dens_int.interpolate(method='linear',\n",
    "                                          limit_direction='both')\n",
    "    df_dens_int = df_dens_int.reset_index()\n",
    "    df_dens_int['index'] = df_dens_int['index'].dt.strftime(\"%Y%m%d\")\n",
    "    df_dens_int = df_dens_int.rename(columns = {'index':'date'})\n",
    "    df_dens_int.to_csv(os.path.join(output_path, 'density_int.csv'), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_coord2ID(data_path, city_list, output_path):\n",
    "    # read each city's transfer.csv and convert coord to region\n",
    "    n_cores = mp.cpu_count() - 1\n",
    "    for city in city_list:\n",
    "        filename = os.path.join(data_path, \"city_%s\" % city, \"transfer.csv\")\n",
    "        df = pd.read_csv(filename,\n",
    "                        header=None,\n",
    "                        names=['hour', 'start_x', 'start_y', 'end_x', 'end_y', 'index'])\n",
    "        \n",
    "        df['city'] = city\n",
    "        # start region\n",
    "        df = df.rename(columns={'start_x':'grid_x', 'start_y':'grid_y'})\n",
    "        df = parallelize_dataframe(df, make_region, n_cores=n_cores)\n",
    "        df = df.rename(columns={'region':'s_region'})\n",
    "        df = df.drop(columns=['grid_x', 'grid_y'])\n",
    "        # end region\n",
    "        df = df.rename(columns={'end_x':'grid_x', 'end_y':'grid_y'})\n",
    "        df = parallelize_dataframe(df, make_region, n_cores=n_cores) \n",
    "        df = df.rename(columns={'region':'e_region'})\n",
    "        df = df.drop(columns=['grid_x', 'grid_y'])\n",
    "        # sorting and saving\n",
    "        df['s_for_sort'] = df['s_region'].str.split('_', expand=True, n=1)[1].astype('int')\n",
    "        df['e_for_sort'] = df['e_region'].str.split('_', expand=True, n=1)[1].astype('int')\n",
    "        df = df.sort_values(['hour', 's_for_sort', 'e_for_sort'])\n",
    "        df = df[['hour', 's_region', 'e_region', 'index']]\n",
    "        df.to_csv(os.path.join(output_path, f'transfer_{city}.csv'), \n",
    "                  index=None, \n",
    "                  header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_index(city_list, output_path):\n",
    "    df_dens = pd.read_csv(os.path.join(output_path,f\"density.csv\"))\n",
    "    df_dens = df_dens.replace({'date':date_idx}).set_index('date')\n",
    "    df_dens.index.name = None\n",
    "    population_dict = df_dens[:6].median(axis=0).to_dict()\n",
    "    # read processed transfer of each city and\n",
    "    # transfer only contains one day's data\n",
    "    dfs = []\n",
    "    for city in city_list:\n",
    "        popu_city = {k:population_dict[k] for k in population_dict.keys() if city in k}\n",
    "        df_popu_city = pd.DataFrame.from_dict(popu_city, orient='index', columns=['popu'])\n",
    "\n",
    "        filename = os.path.join(output_path, f\"transfer_{city}.csv\")\n",
    "        df = pd.read_csv(filename,\n",
    "                        header=None,\n",
    "                        names=['hour', 's_region', 'e_region', 'index'])\n",
    "        # combine points to region\n",
    "        df = df.groupby(['s_region', 'e_region', 'hour'])['index'].sum().reset_index()\n",
    "        # get transfer per hour\n",
    "        df = df.groupby(['s_region', 'e_region'])['index'].median().reset_index()\n",
    "        # sum up all transfers of destination region\n",
    "        df = df.groupby('e_region')['index'].sum().reset_index()\n",
    "        # add population\n",
    "        df = df.merge(df_popu_city, left_on='e_region', right_index=True, how='outer')\n",
    "        # calculate trans_index to population ratio\n",
    "        df['ratio'] = df['index'] / df['popu']\n",
    "        ratio_median = df['ratio'].median()\n",
    "        # fill NaN in index\n",
    "        df['index'] = df['index'].fillna(ratio_median * df['popu'])\n",
    "        # normalize each region's transfer so they sum up to 1\n",
    "        df['index'] = df['index'] / df['index'].sum()\n",
    "        df = df[['e_region', 'index']].rename(columns={'e_region': 'region'})\n",
    "\n",
    "        dfs.append(df)\n",
    "        \n",
    "    df_tr = pd.concat(dfs)\n",
    "    df_tr.to_csv(os.path.join(output_path, 'transfer.csv'), index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_city_migration(data_path, city_list, output_path):\n",
    "    dfs = []\n",
    "    for city_name in city_list:\n",
    "        filename = os.path.join(data_path, \"city_%s\" % city_name, \"migration.csv\")\n",
    "        df = pd.read_csv(filename, \n",
    "                                sep=',', \n",
    "                                header=None,\n",
    "                                names=['date', 's_city', 'e_city', 'index'])\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.groupby(['date', 's_city', 'e_city'])['index'].median().reset_index()\n",
    "#     return df\n",
    "    df.to_csv(os.path.join(output_path, 'migration_city.csv'), index=None) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_city_migration(data_path, city_name, mig_in=True):\n",
    "    # read origional city's migration into dataframe\n",
    "    filename = os.path.join(data_path, \"city_%s\" % city_name, \"migration.csv\")\n",
    "    migration = pd.read_csv(filename, \n",
    "                            sep=',', \n",
    "                            header=None,\n",
    "                            names=['date', 's_city', 'e_city', city_name])\n",
    "\n",
    "    # only use moving in \"city\" data, ignore moving out data\n",
    "    if mig_in:\n",
    "        df = migration[migration.e_city == city_name]\n",
    "    else:\n",
    "        df = migration[migration.s_city == city_name]\n",
    "    df = df[[\"date\", city_name]]\n",
    "\n",
    "    # calculate total move in data of \"city\"\n",
    "    df = df.groupby('date')[city_name].sum().reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def migration(data_path, city_list, output_path):\n",
    "    # combine city migration and region transfer\n",
    "    name_tran = os.path.join(output_path, \"transfer.csv\")\n",
    "    df_tran = pd.read_csv(name_tran)\n",
    "\n",
    "    df_in_list = []\n",
    "    df_out_list = []\n",
    "    for city_name in city_list:\n",
    "        df_in = df_tran[df_tran['region'].str.contains(city_name)].copy()\n",
    "        df_out = df_tran[df_tran['region'].str.contains(city_name)].copy()\n",
    "\n",
    "        migration_in = process_city_migration(data_path, city_name, mig_in=True)\n",
    "        migration_out = process_city_migration(data_path, city_name, mig_in=False)\n",
    "        # loop over dates\n",
    "        for i in range(len(migration_in)):\n",
    "            date = migration_in.date[i]\n",
    "\n",
    "            index_mig_in = migration_in[city_name][i]\n",
    "            index_mig_out = migration_out[city_name][i]\n",
    "            df_in[date] = df_in['index'] * index_mig_in\n",
    "            df_out[date] = df_out['index'] * index_mig_out\n",
    "\n",
    "        df_in_list.append(df_in)\n",
    "        df_out_list.append(df_out)\n",
    "\n",
    "    df_in = pd.concat(df_in_list, axis=0).drop(columns=['index'])\n",
    "    df_out = pd.concat(df_out_list, axis=0).drop(columns=['index'])\n",
    "    df_in = df_in.set_index('region').transpose()\n",
    "    df_out = df_out.set_index('region').transpose()\n",
    "    df_in.index.name = 'date'\n",
    "    df_out.index.name = 'date'\n",
    "\n",
    "    df_in.to_csv(os.path.join(output_path, 'migration_in.csv')) \n",
    "    df_out.to_csv(os.path.join(output_path, 'migration_out.csv')) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infection_process(data_path, city_list, region_nums, output_path):\n",
    "    res_city = []\n",
    "    res_region = []\n",
    "    region_name_list = []\n",
    "    for i, city in enumerate(city_list):\n",
    "        filename = os.path.join(data_path, \"city_%s\" % city, \"infection.csv\")\n",
    "        df_inf = pd.read_csv(filename, \n",
    "                                sep=',', \n",
    "                                header=None,\n",
    "                                names=[\"city\", \"region\", \"date\", \"infect\"])\n",
    "\n",
    "        df_city = df_inf.copy()\n",
    "        df_city = df_city.groupby(['date'])['infect'].sum().reset_index()\n",
    "        df_city = df_city.rename(columns={'infect': city})\n",
    "        res_city.append(df_city)\n",
    "\n",
    "        order = sorted(range(region_nums[i]), key=lambda x:str(x))\n",
    "        for j, idx in enumerate(order):\n",
    "            target_region = idx #str(idx)\n",
    "            df_region = df_inf[df_inf['region'] == target_region].reset_index(drop=True)\n",
    "            if i == 0 and j == 0:\n",
    "                df_region = df_region[['date', 'infect']]\n",
    "            else:\n",
    "                df_region = df_region[['infect']]\n",
    "\n",
    "            df_region = df_region.rename(columns={'infect': '%s_%d' % (city, idx)})\n",
    "            region_name_list.append(\"%s_%d\" % (city, idx))\n",
    "\n",
    "            res_region.append(df_region)\n",
    "\n",
    "    df_region = pd.concat(res_region, axis=1)\n",
    "    df_city = reduce(lambda left, right: pd.merge(left, right, on='date', how='outer'), \n",
    "                     res_city)\n",
    "\n",
    "    region_to_save = os.path.join(output_path, \"infection.csv\")\n",
    "    df_region.to_csv(region_to_save, index=False)\n",
    "\n",
    "    city_to_save = os.path.join(output_path, \"infection_city.csv\")\n",
    "    df_city.to_csv(city_to_save, index=False)\n",
    "\n",
    "    region_name_file = os.path.join(output_path, \"region_names.txt\")\n",
    "    with open(region_name_file, 'w') as f:\n",
    "        names = ' '.join(region_name_list)\n",
    "        f.write(names + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_pred(data_path, city_list, region_nums, output_path):\n",
    "    df = pd.DataFrame()\n",
    "    df['date'] = pd.date_range('2120-05-01', periods=60).strftime('%Y%m%d')\n",
    "    names=['date', 'hour', 'T', 'H', 'W_d', 'W_v', 'W_f', 'cond']\n",
    "    for i, city in enumerate(city_list):\n",
    "        df_weather = pd.read_csv(os.path.join(data_path, f'city_{city}/weather.csv'),\n",
    "                                 header=None, \n",
    "                                 names=names)        \n",
    "        df_weather = df_weather[df_weather['hour']>=5]\n",
    "        \n",
    "        sr_weath = df_weather.groupby('date')['T'].mean().reset_index(drop=True)\n",
    "#         sr_weath_mean = sr_weath.copy()\n",
    "#         sr_weath_mean[:] = sr_weath.mean()\n",
    "        sr_weath_pred = np.zeros(90)\n",
    "        sr_weath_pred[:60] = sr_weath.copy()\n",
    "        sr_weath_pred[60] = sr_weath_pred[58:60].mean()\n",
    "        \n",
    "#         lmd = 0.9   \n",
    "#         sr_weath_stable = sr_weath[:20].median()     \n",
    "#         for j in range(60, len(sr_weath_pred)):\n",
    "#             sr_weath_pred[j] = sr_weath_stable * (1-lmd) + sr_weath_pred[i-1] * lmd  \n",
    "\n",
    "        order = sorted(range(region_nums[i]), key=lambda x:str(x))\n",
    "        for idx in order:\n",
    "            df[f'{city}_{idx}'] = sr_weath\n",
    "\n",
    "    df.to_csv(os.path.join(output_path, 'temperature.csv'), index=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/train_data_all'\n",
    "output_path = './dataset/data_processed'\n",
    "city_list = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
    "\n",
    "region_nums = []\n",
    "for city in city_list:\n",
    "    df = pd.read_csv(os.path.join(data_path, f'city_{city}', 'grid_attr.csv'), header=None, names=['x', 'y', 'region'])\n",
    "    region_nums.append(len(set(df['region'])))\n",
    "    \n",
    "date_range = pd.date_range('2120-05-01', '2120-06-29')\n",
    "date_idx = {int(date.strftime('%Y%m%d')):ind \n",
    "            for date, ind in zip(date_range, range(len(date_range)))}\n",
    "grids_table = Grids(data_path, city_list)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_coord2ID(data_path, city_list, output_path)\n",
    "density_process(data_path, city_list, output_path)\n",
    "transfer_index(city_list, output_path)\n",
    "migration(data_path, city_list, output_path)\n",
    "infection_process(data_path, city_list, region_nums, output_path)\n",
    "combine_city_migration(data_path, city_list, output_path)\n",
    "\n",
    "weather_pred(data_path, city_list, region_nums, output_path)\n",
    "density_int(output_path, date_range)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
